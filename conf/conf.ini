#0:false 1:true
[settings]
train_src_file=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/corpora/timit/mfcc/train.mfcc
train_trg_file=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/corpora/timit/text/train.txt
src_max_len= 1000
trg_max_len= 100
src_dim=13
trg_dim=40
numpy_dir=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/corpora/timit/numpy/batch/
dict=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/corpora/timit/dict/lexicon.txt
[tasks]
src_data_prepare=1
trg_data_prepare=0

[trainer]
batchsize=10
epoch=10
out_dir=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/
resume=0
units=1000
hidden_units=512
mode=train
model=in
att_type=dot
learning_ratio=0.001
lr_descend=1.8
lr_stop=0.000001
depth=1
drop=0
grad_clip=5
bprob_len=35
out_model=0
seed=0


[gpu]
gpu=0
