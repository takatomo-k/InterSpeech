#0:false 1:true
[settings]
train_src_file=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/corpora/tts_gen/fbank/train.fbk
train_trg_file=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/corpora/tts_gen/text/train_60k_ph_num.en
src_max_len= 1500
trg_max_len= 150
src_dim=23
trg_dim=44
numpy_dir=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/corpora/tts_gen/numpy/batch/
dict=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/corpora/tts_gen/dict/tts_lex.txt
[tasks]
data_prepare=1

[trainer]
batchsize=20
epoch=20
out_dir=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/
resume=0
units=1000
hidden_units=512
mode=train
model=in
att_type=dot
learning_ratio=0.01
lr_descend=1.8
lr_stop=0.000001
depth=1
drop=0
grad_clip=5
bprob_len=35
out_model=0
seed=0
log_dir=/project/nakamura-lab05/Work/takatomo-k/InterSpeech/log1-1/

[gpu]
gpu=0
